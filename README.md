# Biomedical Language Processing with Instruction Tuning (Llama2)
Welcome to the QLora project, a biomedical language processing model based on instruction tuning. This project is inspired by the research paper "Exploring the Effectiveness of Instruction Tuning in Biomedical Language Processing".

[Link to the article](https://arxiv.org/abs/2401.00579)


## Overview
Large Language Models (LLMs), particularly those similar to ChatGPT, have significantly influenced the field of Natural Language Processing (NLP). While these models excel in general language tasks, their performance in domain-specific downstream tasks such as biomedical and clinical Named Entity Recognition (NER), Relation Extraction (RE), and Medical Natural Language Inference (NLI) is still evolving. 

## Dataset 
[nlpie/Llama2-MedTuned-Instructions](https://huggingface.co/datasets/nlpie/Llama2-MedTuned-Instructions).

## Schematic representation of the model :
### model:
![image](https://github.com/almog2290/Instruction_Tuning_MedLlama2/assets/25738160/9a1d0d02-7ba8-498c-8203-f0b62667800f)

### Training transformer concept : 
![image](https://github.com/almog2290/Instruction_Tuning_MedLlama2/assets/25738160/dfb5e1d9-3b9e-44b2-a800-51e0ba754cf5)

### Inference transformer concept : 
![image](https://github.com/almog2290/Instruction_Tuning_MedLlama2/assets/25738160/79901bcb-ff47-4cbc-bd7f-37f7e311e2e3)

## Implementation

The implementation of the project was carried out using the Kaggle platform, which provides a remote and computational work environment for data analysis and machine learning. This platform allows us to perform the training and testing of the model remotely, while maintaining efficiency and speed.
